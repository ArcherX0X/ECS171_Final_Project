{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArcherX0X/ECS171_Final_Project/blob/main/ECS171__Final_Project_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GynFb_tV7k3P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "cDD3kcPvFclK",
        "outputId": "5e2feb2b-3bf6-4602-9210-88b357606ac2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c27ba97-d4b2-4f8b-8d37-4d28e17870f4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c27ba97-d4b2-4f8b-8d37-4d28e17870f4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving features_30_sec.csv to features_30_sec (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "wDchvCL5C8Ek",
        "outputId": "8e64790a-c739-4a3c-b44b-034a3de84ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9990, 60)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
              "0          0.335406         0.091048  0.130405  0.003521   \n",
              "1          0.343065         0.086147  0.112699  0.001450   \n",
              "2          0.346815         0.092243  0.132003  0.004620   \n",
              "3          0.363639         0.086856  0.132565  0.002448   \n",
              "4          0.335579         0.088129  0.143289  0.001701   \n",
              "5          0.376670         0.089702  0.132618  0.003583   \n",
              "6          0.379909         0.088827  0.130335  0.003166   \n",
              "7          0.331880         0.092119  0.140600  0.002546   \n",
              "8          0.347877         0.094209  0.133130  0.002538   \n",
              "9          0.358061         0.082957  0.115312  0.001846   \n",
              "\n",
              "   spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
              "0             1773.065032          167541.630869              1972.744388   \n",
              "1             1816.693777           90525.690866              2010.051501   \n",
              "2             1788.539719          111407.437613              2084.565132   \n",
              "3             1655.289045          111952.284517              1960.039988   \n",
              "4             1630.656199           79667.267654              1948.503884   \n",
              "5             1994.915219          211700.619569              2152.767854   \n",
              "6             1962.150096          177443.070045              2146.503479   \n",
              "7             1701.890924           35678.130616              1979.387612   \n",
              "8             1746.473502          138073.931244              1887.619723   \n",
              "9             1763.948942           61493.423121              1874.195710   \n",
              "\n",
              "   spectral_bandwidth_var  rolloff_mean   rolloff_var  ...  mfcc16_mean  \\\n",
              "0           117335.771563   3714.560359  1.080790e+06  ...    -2.853603   \n",
              "1            65671.875673   3869.682242  6.722448e+05  ...     4.074709   \n",
              "2            75124.921716   3997.639160  7.907127e+05  ...     4.806280   \n",
              "3            82913.639269   3568.300218  9.216524e+05  ...    -1.359111   \n",
              "4            60204.020268   3469.992864  6.102111e+05  ...     2.092937   \n",
              "5            74263.873102   4371.985614  1.067105e+06  ...     2.127198   \n",
              "6            98020.541422   4325.026668  1.172363e+06  ...     4.893660   \n",
              "7            36670.725886   3625.280386  3.174617e+05  ...     3.122667   \n",
              "8           117069.920049   3586.934721  1.057633e+06  ...    -0.285936   \n",
              "9            51944.921435   3505.522649  4.451579e+05  ...     6.691686   \n",
              "\n",
              "   mfcc16_var  mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
              "0   39.687145    -3.241280   36.488243     0.722209   38.099152    -5.050335   \n",
              "1   64.748276    -6.055294   40.677654     0.159015   51.264091    -2.837699   \n",
              "2   67.336563    -1.768610   28.348579     2.378768   45.717648    -1.938424   \n",
              "3   47.739452    -3.841155   28.337118     1.218588   34.770935    -3.580352   \n",
              "4   30.336359     0.664582   45.880913     1.689446   51.363583    -3.392489   \n",
              "5   31.448069    -3.448373   34.284130    -0.416165   40.791092    -3.649625   \n",
              "6   33.954071    -2.068194   25.623655     1.428141   47.957699    -3.267124   \n",
              "7   38.456211    -3.637886   24.530296    -0.105148   26.716150    -2.016985   \n",
              "8   44.311455    -4.370029   29.873167     2.114592   33.843155    -2.264663   \n",
              "9   43.967834    -3.448304   48.671944     0.099792   41.839546    -7.677177   \n",
              "\n",
              "   mfcc19_var  mfcc20_mean  mfcc20_var  \n",
              "0   33.618073    -0.243027   43.771767  \n",
              "1   97.030830     5.784063   59.943081  \n",
              "2   53.050835     2.517375   33.105122  \n",
              "3   50.836224     3.630866   32.023678  \n",
              "4   26.738789     0.536961   29.146694  \n",
              "5   32.457901     3.025218   28.892687  \n",
              "6   39.382240     3.276939   25.999132  \n",
              "7   23.150423     0.210787   42.512966  \n",
              "8   80.812393     3.758598   97.618835  \n",
              "9   96.253654     0.791776   40.416420  \n",
              "\n",
              "[10 rows x 57 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8569ca69-060d-4143-a807-8a8b9d9fb69b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chroma_stft_mean</th>\n",
              "      <th>chroma_stft_var</th>\n",
              "      <th>rms_mean</th>\n",
              "      <th>rms_var</th>\n",
              "      <th>spectral_centroid_mean</th>\n",
              "      <th>spectral_centroid_var</th>\n",
              "      <th>spectral_bandwidth_mean</th>\n",
              "      <th>spectral_bandwidth_var</th>\n",
              "      <th>rolloff_mean</th>\n",
              "      <th>rolloff_var</th>\n",
              "      <th>...</th>\n",
              "      <th>mfcc16_mean</th>\n",
              "      <th>mfcc16_var</th>\n",
              "      <th>mfcc17_mean</th>\n",
              "      <th>mfcc17_var</th>\n",
              "      <th>mfcc18_mean</th>\n",
              "      <th>mfcc18_var</th>\n",
              "      <th>mfcc19_mean</th>\n",
              "      <th>mfcc19_var</th>\n",
              "      <th>mfcc20_mean</th>\n",
              "      <th>mfcc20_var</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.335406</td>\n",
              "      <td>0.091048</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>1773.065032</td>\n",
              "      <td>167541.630869</td>\n",
              "      <td>1972.744388</td>\n",
              "      <td>117335.771563</td>\n",
              "      <td>3714.560359</td>\n",
              "      <td>1.080790e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.853603</td>\n",
              "      <td>39.687145</td>\n",
              "      <td>-3.241280</td>\n",
              "      <td>36.488243</td>\n",
              "      <td>0.722209</td>\n",
              "      <td>38.099152</td>\n",
              "      <td>-5.050335</td>\n",
              "      <td>33.618073</td>\n",
              "      <td>-0.243027</td>\n",
              "      <td>43.771767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.343065</td>\n",
              "      <td>0.086147</td>\n",
              "      <td>0.112699</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>1816.693777</td>\n",
              "      <td>90525.690866</td>\n",
              "      <td>2010.051501</td>\n",
              "      <td>65671.875673</td>\n",
              "      <td>3869.682242</td>\n",
              "      <td>6.722448e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>4.074709</td>\n",
              "      <td>64.748276</td>\n",
              "      <td>-6.055294</td>\n",
              "      <td>40.677654</td>\n",
              "      <td>0.159015</td>\n",
              "      <td>51.264091</td>\n",
              "      <td>-2.837699</td>\n",
              "      <td>97.030830</td>\n",
              "      <td>5.784063</td>\n",
              "      <td>59.943081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.346815</td>\n",
              "      <td>0.092243</td>\n",
              "      <td>0.132003</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>1788.539719</td>\n",
              "      <td>111407.437613</td>\n",
              "      <td>2084.565132</td>\n",
              "      <td>75124.921716</td>\n",
              "      <td>3997.639160</td>\n",
              "      <td>7.907127e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>4.806280</td>\n",
              "      <td>67.336563</td>\n",
              "      <td>-1.768610</td>\n",
              "      <td>28.348579</td>\n",
              "      <td>2.378768</td>\n",
              "      <td>45.717648</td>\n",
              "      <td>-1.938424</td>\n",
              "      <td>53.050835</td>\n",
              "      <td>2.517375</td>\n",
              "      <td>33.105122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.363639</td>\n",
              "      <td>0.086856</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>1655.289045</td>\n",
              "      <td>111952.284517</td>\n",
              "      <td>1960.039988</td>\n",
              "      <td>82913.639269</td>\n",
              "      <td>3568.300218</td>\n",
              "      <td>9.216524e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.359111</td>\n",
              "      <td>47.739452</td>\n",
              "      <td>-3.841155</td>\n",
              "      <td>28.337118</td>\n",
              "      <td>1.218588</td>\n",
              "      <td>34.770935</td>\n",
              "      <td>-3.580352</td>\n",
              "      <td>50.836224</td>\n",
              "      <td>3.630866</td>\n",
              "      <td>32.023678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.335579</td>\n",
              "      <td>0.088129</td>\n",
              "      <td>0.143289</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>1630.656199</td>\n",
              "      <td>79667.267654</td>\n",
              "      <td>1948.503884</td>\n",
              "      <td>60204.020268</td>\n",
              "      <td>3469.992864</td>\n",
              "      <td>6.102111e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>2.092937</td>\n",
              "      <td>30.336359</td>\n",
              "      <td>0.664582</td>\n",
              "      <td>45.880913</td>\n",
              "      <td>1.689446</td>\n",
              "      <td>51.363583</td>\n",
              "      <td>-3.392489</td>\n",
              "      <td>26.738789</td>\n",
              "      <td>0.536961</td>\n",
              "      <td>29.146694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.376670</td>\n",
              "      <td>0.089702</td>\n",
              "      <td>0.132618</td>\n",
              "      <td>0.003583</td>\n",
              "      <td>1994.915219</td>\n",
              "      <td>211700.619569</td>\n",
              "      <td>2152.767854</td>\n",
              "      <td>74263.873102</td>\n",
              "      <td>4371.985614</td>\n",
              "      <td>1.067105e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>2.127198</td>\n",
              "      <td>31.448069</td>\n",
              "      <td>-3.448373</td>\n",
              "      <td>34.284130</td>\n",
              "      <td>-0.416165</td>\n",
              "      <td>40.791092</td>\n",
              "      <td>-3.649625</td>\n",
              "      <td>32.457901</td>\n",
              "      <td>3.025218</td>\n",
              "      <td>28.892687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.379909</td>\n",
              "      <td>0.088827</td>\n",
              "      <td>0.130335</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>1962.150096</td>\n",
              "      <td>177443.070045</td>\n",
              "      <td>2146.503479</td>\n",
              "      <td>98020.541422</td>\n",
              "      <td>4325.026668</td>\n",
              "      <td>1.172363e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>4.893660</td>\n",
              "      <td>33.954071</td>\n",
              "      <td>-2.068194</td>\n",
              "      <td>25.623655</td>\n",
              "      <td>1.428141</td>\n",
              "      <td>47.957699</td>\n",
              "      <td>-3.267124</td>\n",
              "      <td>39.382240</td>\n",
              "      <td>3.276939</td>\n",
              "      <td>25.999132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.331880</td>\n",
              "      <td>0.092119</td>\n",
              "      <td>0.140600</td>\n",
              "      <td>0.002546</td>\n",
              "      <td>1701.890924</td>\n",
              "      <td>35678.130616</td>\n",
              "      <td>1979.387612</td>\n",
              "      <td>36670.725886</td>\n",
              "      <td>3625.280386</td>\n",
              "      <td>3.174617e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>3.122667</td>\n",
              "      <td>38.456211</td>\n",
              "      <td>-3.637886</td>\n",
              "      <td>24.530296</td>\n",
              "      <td>-0.105148</td>\n",
              "      <td>26.716150</td>\n",
              "      <td>-2.016985</td>\n",
              "      <td>23.150423</td>\n",
              "      <td>0.210787</td>\n",
              "      <td>42.512966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.347877</td>\n",
              "      <td>0.094209</td>\n",
              "      <td>0.133130</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>1746.473502</td>\n",
              "      <td>138073.931244</td>\n",
              "      <td>1887.619723</td>\n",
              "      <td>117069.920049</td>\n",
              "      <td>3586.934721</td>\n",
              "      <td>1.057633e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.285936</td>\n",
              "      <td>44.311455</td>\n",
              "      <td>-4.370029</td>\n",
              "      <td>29.873167</td>\n",
              "      <td>2.114592</td>\n",
              "      <td>33.843155</td>\n",
              "      <td>-2.264663</td>\n",
              "      <td>80.812393</td>\n",
              "      <td>3.758598</td>\n",
              "      <td>97.618835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.358061</td>\n",
              "      <td>0.082957</td>\n",
              "      <td>0.115312</td>\n",
              "      <td>0.001846</td>\n",
              "      <td>1763.948942</td>\n",
              "      <td>61493.423121</td>\n",
              "      <td>1874.195710</td>\n",
              "      <td>51944.921435</td>\n",
              "      <td>3505.522649</td>\n",
              "      <td>4.451579e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>6.691686</td>\n",
              "      <td>43.967834</td>\n",
              "      <td>-3.448304</td>\n",
              "      <td>48.671944</td>\n",
              "      <td>0.099792</td>\n",
              "      <td>41.839546</td>\n",
              "      <td>-7.677177</td>\n",
              "      <td>96.253654</td>\n",
              "      <td>0.791776</td>\n",
              "      <td>40.416420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 57 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8569ca69-060d-4143-a807-8a8b9d9fb69b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8569ca69-060d-4143-a807-8a8b9d9fb69b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8569ca69-060d-4143-a807-8a8b9d9fb69b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d25fa7ca-f237-4b47-98d3-08d4b5abfaae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d25fa7ca-f237-4b47-98d3-08d4b5abfaae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d25fa7ca-f237-4b47-98d3-08d4b5abfaae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('/content/features_3_sec.csv')\n",
        "print(df.shape)\n",
        "df = df.drop('filename', axis=1)\n",
        "df = df.drop('length', axis=1)\n",
        "y = df.pop('label').values\n",
        "print(type(y))\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9dUSBYPGLCj"
      },
      "outputs": [],
      "source": [
        "# 80:20\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Fits the scaler on the training features\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transforms the training and testing features\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnWDIagwJ1ms"
      },
      "outputs": [],
      "source": [
        "# Encodes labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Converts to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only additional Hidden Layers"
      ],
      "metadata": {
        "id": "s5y-n5mi6Xk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = nn.functional.softmax(x, dim=1)  # softmax\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 512  # First hidden layer\n",
        "hidden_size2 = 256  # Second hidden layer\n",
        "hidden_size3 = 128  # Third hidden layer\n",
        "num_classes = len(label_encoder.classes_)\n",
        "learning_rate = 0.002\n",
        "num_epochs = 200\n",
        "batch_size = 32\n",
        "\n",
        "# Initializes model, loss function, and optimizer\n",
        "model = Classifier(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "8r-FAH3H6dLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c187ce0-e737-40e0-cbf3-9c35361107a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 2.0006\n",
            "Epoch [2/200], Loss: 2.0202\n",
            "Epoch [3/200], Loss: 2.0861\n",
            "Epoch [4/200], Loss: 2.1922\n",
            "Epoch [5/200], Loss: 2.0956\n",
            "Epoch [6/200], Loss: 1.7823\n",
            "Epoch [7/200], Loss: 2.1376\n",
            "Epoch [8/200], Loss: 1.9746\n",
            "Epoch [9/200], Loss: 1.9827\n",
            "Epoch [10/200], Loss: 1.9265\n",
            "Epoch [11/200], Loss: 1.8463\n",
            "Epoch [12/200], Loss: 1.8841\n",
            "Epoch [13/200], Loss: 2.0289\n",
            "Epoch [14/200], Loss: 1.7117\n",
            "Epoch [15/200], Loss: 1.8214\n",
            "Epoch [16/200], Loss: 1.6857\n",
            "Epoch [17/200], Loss: 1.9284\n",
            "Epoch [18/200], Loss: 1.9286\n",
            "Epoch [19/200], Loss: 1.7889\n",
            "Epoch [20/200], Loss: 1.5598\n",
            "Epoch [21/200], Loss: 1.8465\n",
            "Epoch [22/200], Loss: 1.7497\n",
            "Epoch [23/200], Loss: 1.5399\n",
            "Epoch [24/200], Loss: 1.7507\n",
            "Epoch [25/200], Loss: 1.6862\n",
            "Epoch [26/200], Loss: 1.8216\n",
            "Epoch [27/200], Loss: 1.8156\n",
            "Epoch [28/200], Loss: 1.6626\n",
            "Epoch [29/200], Loss: 1.8855\n",
            "Epoch [30/200], Loss: 1.5538\n",
            "Epoch [31/200], Loss: 1.8748\n",
            "Epoch [32/200], Loss: 1.8827\n",
            "Epoch [33/200], Loss: 1.7127\n",
            "Epoch [34/200], Loss: 1.6709\n",
            "Epoch [35/200], Loss: 1.5971\n",
            "Epoch [36/200], Loss: 1.9702\n",
            "Epoch [37/200], Loss: 1.7024\n",
            "Epoch [38/200], Loss: 1.7106\n",
            "Epoch [39/200], Loss: 1.6685\n",
            "Epoch [40/200], Loss: 1.7116\n",
            "Epoch [41/200], Loss: 1.6611\n",
            "Epoch [42/200], Loss: 1.8491\n",
            "Epoch [43/200], Loss: 1.6813\n",
            "Epoch [44/200], Loss: 1.7406\n",
            "Epoch [45/200], Loss: 1.7266\n",
            "Epoch [46/200], Loss: 1.8039\n",
            "Epoch [47/200], Loss: 1.7559\n",
            "Epoch [48/200], Loss: 1.7500\n",
            "Epoch [49/200], Loss: 1.5623\n",
            "Epoch [50/200], Loss: 1.7986\n",
            "Epoch [51/200], Loss: 1.6533\n",
            "Epoch [52/200], Loss: 1.7163\n",
            "Epoch [53/200], Loss: 1.4787\n",
            "Epoch [54/200], Loss: 1.6845\n",
            "Epoch [55/200], Loss: 1.7089\n",
            "Epoch [56/200], Loss: 1.8326\n",
            "Epoch [57/200], Loss: 1.6186\n",
            "Epoch [58/200], Loss: 1.7105\n",
            "Epoch [59/200], Loss: 1.6258\n",
            "Epoch [60/200], Loss: 1.6852\n",
            "Epoch [61/200], Loss: 1.6674\n",
            "Epoch [62/200], Loss: 1.6275\n",
            "Epoch [63/200], Loss: 1.7448\n",
            "Epoch [64/200], Loss: 1.8320\n",
            "Epoch [65/200], Loss: 1.7361\n",
            "Epoch [66/200], Loss: 1.5837\n",
            "Epoch [67/200], Loss: 1.5566\n",
            "Epoch [68/200], Loss: 1.5863\n",
            "Epoch [69/200], Loss: 1.7157\n",
            "Epoch [70/200], Loss: 1.7975\n",
            "Epoch [71/200], Loss: 1.7127\n",
            "Epoch [72/200], Loss: 1.7376\n",
            "Epoch [73/200], Loss: 1.7783\n",
            "Epoch [74/200], Loss: 1.6280\n",
            "Epoch [75/200], Loss: 1.5889\n",
            "Epoch [76/200], Loss: 1.8672\n",
            "Epoch [77/200], Loss: 1.7518\n",
            "Epoch [78/200], Loss: 1.5862\n",
            "Epoch [79/200], Loss: 1.6956\n",
            "Epoch [80/200], Loss: 1.7607\n",
            "Epoch [81/200], Loss: 1.5863\n",
            "Epoch [82/200], Loss: 1.6857\n",
            "Epoch [83/200], Loss: 1.6353\n",
            "Epoch [84/200], Loss: 1.6275\n",
            "Epoch [85/200], Loss: 1.7747\n",
            "Epoch [86/200], Loss: 1.7135\n",
            "Epoch [87/200], Loss: 1.7614\n",
            "Epoch [88/200], Loss: 1.7528\n",
            "Epoch [89/200], Loss: 1.6873\n",
            "Epoch [90/200], Loss: 1.6280\n",
            "Epoch [91/200], Loss: 1.7555\n",
            "Epoch [92/200], Loss: 1.6702\n",
            "Epoch [93/200], Loss: 1.5973\n",
            "Epoch [94/200], Loss: 1.6450\n",
            "Epoch [95/200], Loss: 1.6239\n",
            "Epoch [96/200], Loss: 1.6164\n",
            "Epoch [97/200], Loss: 1.6544\n",
            "Epoch [98/200], Loss: 1.7504\n",
            "Epoch [99/200], Loss: 1.5875\n",
            "Epoch [100/200], Loss: 1.6574\n",
            "Epoch [101/200], Loss: 1.6688\n",
            "Epoch [102/200], Loss: 1.5805\n",
            "Epoch [103/200], Loss: 1.5978\n",
            "Epoch [104/200], Loss: 1.6273\n",
            "Epoch [105/200], Loss: 1.6672\n",
            "Epoch [106/200], Loss: 1.6707\n",
            "Epoch [107/200], Loss: 1.5838\n",
            "Epoch [108/200], Loss: 1.5863\n",
            "Epoch [109/200], Loss: 1.6843\n",
            "Epoch [110/200], Loss: 1.7579\n",
            "Epoch [111/200], Loss: 1.6118\n",
            "Epoch [112/200], Loss: 1.5285\n",
            "Epoch [113/200], Loss: 1.6680\n",
            "Epoch [114/200], Loss: 1.5349\n",
            "Epoch [115/200], Loss: 1.6163\n",
            "Epoch [116/200], Loss: 1.5087\n",
            "Epoch [117/200], Loss: 1.7496\n",
            "Epoch [118/200], Loss: 1.5156\n",
            "Epoch [119/200], Loss: 1.5954\n",
            "Epoch [120/200], Loss: 1.6629\n",
            "Epoch [121/200], Loss: 1.7072\n",
            "Epoch [122/200], Loss: 1.7440\n",
            "Epoch [123/200], Loss: 1.6692\n",
            "Epoch [124/200], Loss: 1.6667\n",
            "Epoch [125/200], Loss: 1.5535\n",
            "Epoch [126/200], Loss: 1.5549\n",
            "Epoch [127/200], Loss: 1.6758\n",
            "Epoch [128/200], Loss: 1.5452\n",
            "Epoch [129/200], Loss: 1.5448\n",
            "Epoch [130/200], Loss: 1.5842\n",
            "Epoch [131/200], Loss: 1.6273\n",
            "Epoch [132/200], Loss: 1.6682\n",
            "Epoch [133/200], Loss: 1.7052\n",
            "Epoch [134/200], Loss: 1.6348\n",
            "Epoch [135/200], Loss: 1.6577\n",
            "Epoch [136/200], Loss: 1.5849\n",
            "Epoch [137/200], Loss: 1.6244\n",
            "Epoch [138/200], Loss: 1.6701\n",
            "Epoch [139/200], Loss: 1.5494\n",
            "Epoch [140/200], Loss: 1.6768\n",
            "Epoch [141/200], Loss: 1.6659\n",
            "Epoch [142/200], Loss: 1.5862\n",
            "Epoch [143/200], Loss: 1.5053\n",
            "Epoch [144/200], Loss: 1.6078\n",
            "Epoch [145/200], Loss: 1.5446\n",
            "Epoch [146/200], Loss: 1.6695\n",
            "Epoch [147/200], Loss: 1.5716\n",
            "Epoch [148/200], Loss: 1.5461\n",
            "Epoch [149/200], Loss: 1.6636\n",
            "Epoch [150/200], Loss: 1.5097\n",
            "Epoch [151/200], Loss: 1.6748\n",
            "Epoch [152/200], Loss: 1.6274\n",
            "Epoch [153/200], Loss: 1.4612\n",
            "Epoch [154/200], Loss: 1.5858\n",
            "Epoch [155/200], Loss: 1.5445\n",
            "Epoch [156/200], Loss: 1.5878\n",
            "Epoch [157/200], Loss: 1.5132\n",
            "Epoch [158/200], Loss: 1.7203\n",
            "Epoch [159/200], Loss: 1.5881\n",
            "Epoch [160/200], Loss: 1.6208\n",
            "Epoch [161/200], Loss: 1.4948\n",
            "Epoch [162/200], Loss: 1.5614\n",
            "Epoch [163/200], Loss: 1.5432\n",
            "Epoch [164/200], Loss: 1.6888\n",
            "Epoch [165/200], Loss: 1.5841\n",
            "Epoch [166/200], Loss: 1.5860\n",
            "Epoch [167/200], Loss: 1.6300\n",
            "Epoch [168/200], Loss: 1.5787\n",
            "Epoch [169/200], Loss: 1.6278\n",
            "Epoch [170/200], Loss: 1.7102\n",
            "Epoch [171/200], Loss: 1.5862\n",
            "Epoch [172/200], Loss: 1.6307\n",
            "Epoch [173/200], Loss: 1.7063\n",
            "Epoch [174/200], Loss: 1.6268\n",
            "Epoch [175/200], Loss: 1.6100\n",
            "Epoch [176/200], Loss: 1.5474\n",
            "Epoch [177/200], Loss: 1.6289\n",
            "Epoch [178/200], Loss: 1.5046\n",
            "Epoch [179/200], Loss: 1.5984\n",
            "Epoch [180/200], Loss: 1.6770\n",
            "Epoch [181/200], Loss: 1.5029\n",
            "Epoch [182/200], Loss: 1.5467\n",
            "Epoch [183/200], Loss: 1.5855\n",
            "Epoch [184/200], Loss: 1.6350\n",
            "Epoch [185/200], Loss: 1.5751\n",
            "Epoch [186/200], Loss: 1.6066\n",
            "Epoch [187/200], Loss: 1.6662\n",
            "Epoch [188/200], Loss: 1.4612\n",
            "Epoch [189/200], Loss: 1.6421\n",
            "Epoch [190/200], Loss: 1.5028\n",
            "Epoch [191/200], Loss: 1.5518\n",
            "Epoch [192/200], Loss: 1.5445\n",
            "Epoch [193/200], Loss: 1.5862\n",
            "Epoch [194/200], Loss: 1.5669\n",
            "Epoch [195/200], Loss: 1.5468\n",
            "Epoch [196/200], Loss: 1.5853\n",
            "Epoch [197/200], Loss: 1.5442\n",
            "Epoch [198/200], Loss: 1.5025\n",
            "Epoch [199/200], Loss: 1.6817\n",
            "Epoch [200/200], Loss: 1.5117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    probabilities, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    y_test_decoded = label_encoder.inverse_transform(y_test_tensor)\n",
        "    predicted_decoded = label_encoder.inverse_transform(predicted)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test_decoded, predicted_decoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEgQPTjc6mlH",
        "outputId": "6fab240d-f845-47a9-9a90-166e48232ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7913\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.91      0.66      0.77       208\n",
            "   classical       0.87      0.98      0.92       203\n",
            "     country       0.58      0.85      0.69       186\n",
            "       disco       0.75      0.76      0.75       199\n",
            "      hiphop       0.97      0.62      0.76       218\n",
            "        jazz       0.86      0.80      0.83       192\n",
            "       metal       0.89      0.88      0.89       204\n",
            "         pop       0.76      0.93      0.83       180\n",
            "      reggae       0.75      0.79      0.77       211\n",
            "        rock       0.73      0.68      0.70       197\n",
            "\n",
            "    accuracy                           0.79      1998\n",
            "   macro avg       0.81      0.79      0.79      1998\n",
            "weighted avg       0.81      0.79      0.79      1998\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With additional Hidden Layers + DropOut"
      ],
      "metadata": {
        "id": "j3DZKrCHikj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes, dropout_rate=0.5):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)  # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)  # Apply dropout\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)  # Apply dropout\n",
        "        x = self.fc4(x)\n",
        "        x = nn.functional.softmax(x, dim=1)  # softmax\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OUyMHuxEiSVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 512  # First hidden layer\n",
        "hidden_size2 = 256  # Second hidden layer\n",
        "hidden_size3 = 128  # Third hidden layer\n",
        "num_classes = len(label_encoder.classes_)\n",
        "learning_rate = 0.002\n",
        "num_epochs = 200\n",
        "batch_size = 32\n",
        "\n",
        "# Initializes model, loss function, and optimizer\n",
        "model = Classifier(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Evaluates the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    probabilities, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    y_test_decoded = label_encoder.inverse_transform(y_test_tensor)\n",
        "    predicted_decoded = label_encoder.inverse_transform(predicted)\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test_decoded, predicted_decoded))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLG4s-fSiwMK",
        "outputId": "81d527cd-b61e-460d-d6d4-36730da1c6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 2.1257\n",
            "Epoch [2/200], Loss: 2.1336\n",
            "Epoch [3/200], Loss: 2.0260\n",
            "Epoch [4/200], Loss: 1.8987\n",
            "Epoch [5/200], Loss: 2.0662\n",
            "Epoch [6/200], Loss: 2.1309\n",
            "Epoch [7/200], Loss: 2.1642\n",
            "Epoch [8/200], Loss: 1.9369\n",
            "Epoch [9/200], Loss: 1.8597\n",
            "Epoch [10/200], Loss: 2.0145\n",
            "Epoch [11/200], Loss: 2.0217\n",
            "Epoch [12/200], Loss: 1.8480\n",
            "Epoch [13/200], Loss: 1.9279\n",
            "Epoch [14/200], Loss: 1.9764\n",
            "Epoch [15/200], Loss: 1.9500\n",
            "Epoch [16/200], Loss: 2.0690\n",
            "Epoch [17/200], Loss: 1.9853\n",
            "Epoch [18/200], Loss: 1.7095\n",
            "Epoch [19/200], Loss: 2.0744\n",
            "Epoch [20/200], Loss: 1.8187\n",
            "Epoch [21/200], Loss: 1.9269\n",
            "Epoch [22/200], Loss: 2.0394\n",
            "Epoch [23/200], Loss: 1.8650\n",
            "Epoch [24/200], Loss: 2.0541\n",
            "Epoch [25/200], Loss: 1.9005\n",
            "Epoch [26/200], Loss: 1.9457\n",
            "Epoch [27/200], Loss: 2.0803\n",
            "Epoch [28/200], Loss: 1.9516\n",
            "Epoch [29/200], Loss: 2.1116\n",
            "Epoch [30/200], Loss: 1.8754\n",
            "Epoch [31/200], Loss: 1.9299\n",
            "Epoch [32/200], Loss: 1.7957\n",
            "Epoch [33/200], Loss: 1.9605\n",
            "Epoch [34/200], Loss: 1.9565\n",
            "Epoch [35/200], Loss: 1.9192\n",
            "Epoch [36/200], Loss: 1.8644\n",
            "Epoch [37/200], Loss: 1.6915\n",
            "Epoch [38/200], Loss: 1.9588\n",
            "Epoch [39/200], Loss: 1.8201\n",
            "Epoch [40/200], Loss: 2.1848\n",
            "Epoch [41/200], Loss: 1.9973\n",
            "Epoch [42/200], Loss: 1.7876\n",
            "Epoch [43/200], Loss: 1.9293\n",
            "Epoch [44/200], Loss: 1.9621\n",
            "Epoch [45/200], Loss: 2.0122\n",
            "Epoch [46/200], Loss: 1.8893\n",
            "Epoch [47/200], Loss: 1.7514\n",
            "Epoch [48/200], Loss: 1.9193\n",
            "Epoch [49/200], Loss: 1.8762\n",
            "Epoch [50/200], Loss: 2.0040\n",
            "Epoch [51/200], Loss: 1.9174\n",
            "Epoch [52/200], Loss: 1.8234\n",
            "Epoch [53/200], Loss: 2.2492\n",
            "Epoch [54/200], Loss: 2.0848\n",
            "Epoch [55/200], Loss: 1.8360\n",
            "Epoch [56/200], Loss: 2.0381\n",
            "Epoch [57/200], Loss: 1.8736\n",
            "Epoch [58/200], Loss: 1.8663\n",
            "Epoch [59/200], Loss: 2.0573\n",
            "Epoch [60/200], Loss: 1.8796\n",
            "Epoch [61/200], Loss: 1.8146\n",
            "Epoch [62/200], Loss: 1.9358\n",
            "Epoch [63/200], Loss: 1.9041\n",
            "Epoch [64/200], Loss: 2.1112\n",
            "Epoch [65/200], Loss: 1.9590\n",
            "Epoch [66/200], Loss: 2.0221\n",
            "Epoch [67/200], Loss: 1.7526\n",
            "Epoch [68/200], Loss: 1.8494\n",
            "Epoch [69/200], Loss: 2.1271\n",
            "Epoch [70/200], Loss: 1.9589\n",
            "Epoch [71/200], Loss: 1.9056\n",
            "Epoch [72/200], Loss: 1.9194\n",
            "Epoch [73/200], Loss: 1.9508\n",
            "Epoch [74/200], Loss: 1.9758\n",
            "Epoch [75/200], Loss: 2.0028\n",
            "Epoch [76/200], Loss: 1.8766\n",
            "Epoch [77/200], Loss: 1.8342\n",
            "Epoch [78/200], Loss: 1.8792\n",
            "Epoch [79/200], Loss: 1.8956\n",
            "Epoch [80/200], Loss: 2.0064\n",
            "Epoch [81/200], Loss: 1.8775\n",
            "Epoch [82/200], Loss: 1.7531\n",
            "Epoch [83/200], Loss: 2.0007\n",
            "Epoch [84/200], Loss: 1.9567\n",
            "Epoch [85/200], Loss: 2.0027\n",
            "Epoch [86/200], Loss: 1.9673\n",
            "Epoch [87/200], Loss: 1.7954\n",
            "Epoch [88/200], Loss: 2.0445\n",
            "Epoch [89/200], Loss: 1.9278\n",
            "Epoch [90/200], Loss: 2.0568\n",
            "Epoch [91/200], Loss: 2.0860\n",
            "Epoch [92/200], Loss: 1.7957\n",
            "Epoch [93/200], Loss: 1.9188\n",
            "Epoch [94/200], Loss: 1.9197\n",
            "Epoch [95/200], Loss: 1.7941\n",
            "Epoch [96/200], Loss: 2.1240\n",
            "Epoch [97/200], Loss: 1.9729\n",
            "Epoch [98/200], Loss: 1.9181\n",
            "Epoch [99/200], Loss: 2.0445\n",
            "Epoch [100/200], Loss: 1.8329\n",
            "Epoch [101/200], Loss: 2.0411\n",
            "Epoch [102/200], Loss: 2.0431\n",
            "Epoch [103/200], Loss: 2.0851\n",
            "Epoch [104/200], Loss: 1.8356\n",
            "Epoch [105/200], Loss: 2.0032\n",
            "Epoch [106/200], Loss: 2.1278\n",
            "Epoch [107/200], Loss: 1.8774\n",
            "Epoch [108/200], Loss: 1.9607\n",
            "Epoch [109/200], Loss: 2.1386\n",
            "Epoch [110/200], Loss: 1.9546\n",
            "Epoch [111/200], Loss: 1.9833\n",
            "Epoch [112/200], Loss: 1.9583\n",
            "Epoch [113/200], Loss: 1.7945\n",
            "Epoch [114/200], Loss: 1.8794\n",
            "Epoch [115/200], Loss: 2.0112\n",
            "Epoch [116/200], Loss: 1.9962\n",
            "Epoch [117/200], Loss: 1.9165\n",
            "Epoch [118/200], Loss: 1.9588\n",
            "Epoch [119/200], Loss: 2.0172\n",
            "Epoch [120/200], Loss: 2.0043\n",
            "Epoch [121/200], Loss: 2.1278\n",
            "Epoch [122/200], Loss: 2.0861\n",
            "Epoch [123/200], Loss: 2.0232\n",
            "Epoch [124/200], Loss: 1.7943\n",
            "Epoch [125/200], Loss: 2.0445\n",
            "Epoch [126/200], Loss: 1.9196\n",
            "Epoch [127/200], Loss: 2.1514\n",
            "Epoch [128/200], Loss: 2.1460\n",
            "Epoch [129/200], Loss: 1.9181\n",
            "Epoch [130/200], Loss: 1.9991\n",
            "Epoch [131/200], Loss: 1.9195\n",
            "Epoch [132/200], Loss: 2.0028\n",
            "Epoch [133/200], Loss: 1.9195\n",
            "Epoch [134/200], Loss: 2.0526\n",
            "Epoch [135/200], Loss: 1.8086\n",
            "Epoch [136/200], Loss: 2.0428\n",
            "Epoch [137/200], Loss: 1.7945\n",
            "Epoch [138/200], Loss: 1.7945\n",
            "Epoch [139/200], Loss: 2.1720\n",
            "Epoch [140/200], Loss: 2.0833\n",
            "Epoch [141/200], Loss: 2.0028\n",
            "Epoch [142/200], Loss: 2.0845\n",
            "Epoch [143/200], Loss: 2.1278\n",
            "Epoch [144/200], Loss: 2.1247\n",
            "Epoch [145/200], Loss: 2.0032\n",
            "Epoch [146/200], Loss: 1.8070\n",
            "Epoch [147/200], Loss: 2.1278\n",
            "Epoch [148/200], Loss: 2.0027\n",
            "Epoch [149/200], Loss: 1.9612\n",
            "Epoch [150/200], Loss: 1.9195\n",
            "Epoch [151/200], Loss: 1.9574\n",
            "Epoch [152/200], Loss: 1.9195\n",
            "Epoch [153/200], Loss: 2.0445\n",
            "Epoch [154/200], Loss: 2.0445\n",
            "Epoch [155/200], Loss: 1.9606\n",
            "Epoch [156/200], Loss: 2.0862\n",
            "Epoch [157/200], Loss: 2.0781\n",
            "Epoch [158/200], Loss: 1.9195\n",
            "Epoch [159/200], Loss: 2.0862\n",
            "Epoch [160/200], Loss: 1.9658\n",
            "Epoch [161/200], Loss: 2.1685\n",
            "Epoch [162/200], Loss: 2.0862\n",
            "Epoch [163/200], Loss: 2.0440\n",
            "Epoch [164/200], Loss: 2.1278\n",
            "Epoch [165/200], Loss: 1.9195\n",
            "Epoch [166/200], Loss: 1.7945\n",
            "Epoch [167/200], Loss: 2.0862\n",
            "Epoch [168/200], Loss: 1.9612\n",
            "Epoch [169/200], Loss: 2.0861\n",
            "Epoch [170/200], Loss: 2.0028\n",
            "Epoch [171/200], Loss: 2.0862\n",
            "Epoch [172/200], Loss: 2.1159\n",
            "Epoch [173/200], Loss: 2.0028\n",
            "Epoch [174/200], Loss: 2.0443\n",
            "Epoch [175/200], Loss: 2.0439\n",
            "Epoch [176/200], Loss: 2.0029\n",
            "Epoch [177/200], Loss: 1.9612\n",
            "Epoch [178/200], Loss: 2.2111\n",
            "Epoch [179/200], Loss: 2.0027\n",
            "Epoch [180/200], Loss: 2.1267\n",
            "Epoch [181/200], Loss: 2.0028\n",
            "Epoch [182/200], Loss: 1.9611\n",
            "Epoch [183/200], Loss: 1.9204\n",
            "Epoch [184/200], Loss: 1.9195\n",
            "Epoch [185/200], Loss: 2.0858\n",
            "Epoch [186/200], Loss: 2.0028\n",
            "Epoch [187/200], Loss: 2.0028\n",
            "Epoch [188/200], Loss: 1.9612\n",
            "Epoch [189/200], Loss: 1.8778\n",
            "Epoch [190/200], Loss: 2.0027\n",
            "Epoch [191/200], Loss: 1.7218\n",
            "Epoch [192/200], Loss: 1.8778\n",
            "Epoch [193/200], Loss: 2.1278\n",
            "Epoch [194/200], Loss: 1.9547\n",
            "Epoch [195/200], Loss: 1.9611\n",
            "Epoch [196/200], Loss: 1.8778\n",
            "Epoch [197/200], Loss: 2.0445\n",
            "Epoch [198/200], Loss: 2.1270\n",
            "Epoch [199/200], Loss: 2.0434\n",
            "Epoch [200/200], Loss: 2.0445\n",
            "Accuracy: 0.4875\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.33      0.06      0.10       208\n",
            "   classical       0.94      0.74      0.83       203\n",
            "     country       0.71      0.16      0.26       186\n",
            "       disco       0.43      0.08      0.13       199\n",
            "      hiphop       0.48      0.50      0.49       218\n",
            "        jazz       0.49      0.77      0.60       192\n",
            "       metal       0.61      0.87      0.72       204\n",
            "         pop       0.38      0.93      0.54       180\n",
            "      reggae       0.35      0.77      0.48       211\n",
            "        rock       0.75      0.02      0.03       197\n",
            "\n",
            "    accuracy                           0.49      1998\n",
            "   macro avg       0.55      0.49      0.42      1998\n",
            "weighted avg       0.55      0.49      0.42      1998\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Version"
      ],
      "metadata": {
        "id": "JnUby8t3ioDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWeFLTh5J7D6"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = nn.functional.softmax(x, dim=1)  # softmax\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP7Cy8nup9ZN",
        "outputId": "59cd86f0-ceed-4ef7-de6d-77277d9660c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 2.0383\n",
            "Epoch [2/200], Loss: 1.9397\n",
            "Epoch [3/200], Loss: 1.9136\n",
            "Epoch [4/200], Loss: 1.8296\n",
            "Epoch [5/200], Loss: 1.8861\n",
            "Epoch [6/200], Loss: 1.7510\n",
            "Epoch [7/200], Loss: 1.9606\n",
            "Epoch [8/200], Loss: 1.8541\n",
            "Epoch [9/200], Loss: 1.7889\n",
            "Epoch [10/200], Loss: 1.8757\n",
            "Epoch [11/200], Loss: 1.8331\n",
            "Epoch [12/200], Loss: 1.7500\n",
            "Epoch [13/200], Loss: 1.7985\n",
            "Epoch [14/200], Loss: 1.9090\n",
            "Epoch [15/200], Loss: 1.8284\n",
            "Epoch [16/200], Loss: 1.7949\n",
            "Epoch [17/200], Loss: 1.6542\n",
            "Epoch [18/200], Loss: 1.7008\n",
            "Epoch [19/200], Loss: 1.7745\n",
            "Epoch [20/200], Loss: 1.7194\n",
            "Epoch [21/200], Loss: 1.8940\n",
            "Epoch [22/200], Loss: 1.7569\n",
            "Epoch [23/200], Loss: 1.7152\n",
            "Epoch [24/200], Loss: 1.5879\n",
            "Epoch [25/200], Loss: 1.7896\n",
            "Epoch [26/200], Loss: 1.7216\n",
            "Epoch [27/200], Loss: 1.6365\n",
            "Epoch [28/200], Loss: 1.6437\n",
            "Epoch [29/200], Loss: 1.6816\n",
            "Epoch [30/200], Loss: 1.6761\n",
            "Epoch [31/200], Loss: 1.7647\n",
            "Epoch [32/200], Loss: 1.5677\n",
            "Epoch [33/200], Loss: 1.9188\n",
            "Epoch [34/200], Loss: 1.5937\n",
            "Epoch [35/200], Loss: 1.6872\n",
            "Epoch [36/200], Loss: 1.7384\n",
            "Epoch [37/200], Loss: 1.7863\n",
            "Epoch [38/200], Loss: 1.6900\n",
            "Epoch [39/200], Loss: 1.7963\n",
            "Epoch [40/200], Loss: 1.8575\n",
            "Epoch [41/200], Loss: 1.7787\n",
            "Epoch [42/200], Loss: 1.6958\n",
            "Epoch [43/200], Loss: 1.6254\n",
            "Epoch [44/200], Loss: 1.6844\n",
            "Epoch [45/200], Loss: 1.5640\n",
            "Epoch [46/200], Loss: 1.7588\n",
            "Epoch [47/200], Loss: 1.6542\n",
            "Epoch [48/200], Loss: 1.7173\n",
            "Epoch [49/200], Loss: 1.6394\n",
            "Epoch [50/200], Loss: 1.6957\n",
            "Epoch [51/200], Loss: 1.6137\n",
            "Epoch [52/200], Loss: 1.5670\n",
            "Epoch [53/200], Loss: 1.7976\n",
            "Epoch [54/200], Loss: 1.5747\n",
            "Epoch [55/200], Loss: 1.5849\n",
            "Epoch [56/200], Loss: 1.7014\n",
            "Epoch [57/200], Loss: 1.6863\n",
            "Epoch [58/200], Loss: 1.7038\n",
            "Epoch [59/200], Loss: 1.7941\n",
            "Epoch [60/200], Loss: 1.8030\n",
            "Epoch [61/200], Loss: 1.6440\n",
            "Epoch [62/200], Loss: 1.6776\n",
            "Epoch [63/200], Loss: 1.7347\n",
            "Epoch [64/200], Loss: 1.6989\n",
            "Epoch [65/200], Loss: 1.5784\n",
            "Epoch [66/200], Loss: 1.6238\n",
            "Epoch [67/200], Loss: 1.5889\n",
            "Epoch [68/200], Loss: 1.6587\n",
            "Epoch [69/200], Loss: 1.5842\n",
            "Epoch [70/200], Loss: 1.6569\n",
            "Epoch [71/200], Loss: 1.5726\n",
            "Epoch [72/200], Loss: 1.6106\n",
            "Epoch [73/200], Loss: 1.7305\n",
            "Epoch [74/200], Loss: 1.6337\n",
            "Epoch [75/200], Loss: 1.6399\n",
            "Epoch [76/200], Loss: 1.6239\n",
            "Epoch [77/200], Loss: 1.7633\n",
            "Epoch [78/200], Loss: 1.7083\n",
            "Epoch [79/200], Loss: 1.7207\n",
            "Epoch [80/200], Loss: 1.6417\n",
            "Epoch [81/200], Loss: 1.7078\n",
            "Epoch [82/200], Loss: 1.6373\n",
            "Epoch [83/200], Loss: 1.5721\n",
            "Epoch [84/200], Loss: 1.5519\n",
            "Epoch [85/200], Loss: 1.6133\n",
            "Epoch [86/200], Loss: 1.5180\n",
            "Epoch [87/200], Loss: 1.5715\n",
            "Epoch [88/200], Loss: 1.6169\n",
            "Epoch [89/200], Loss: 1.6265\n",
            "Epoch [90/200], Loss: 1.6789\n",
            "Epoch [91/200], Loss: 1.6799\n",
            "Epoch [92/200], Loss: 1.5922\n",
            "Epoch [93/200], Loss: 1.5690\n",
            "Epoch [94/200], Loss: 1.5770\n",
            "Epoch [95/200], Loss: 1.6832\n",
            "Epoch [96/200], Loss: 1.6030\n",
            "Epoch [97/200], Loss: 1.6443\n",
            "Epoch [98/200], Loss: 1.5920\n",
            "Epoch [99/200], Loss: 1.5447\n",
            "Epoch [100/200], Loss: 1.6837\n",
            "Epoch [101/200], Loss: 1.5832\n",
            "Epoch [102/200], Loss: 1.5487\n",
            "Epoch [103/200], Loss: 1.5544\n",
            "Epoch [104/200], Loss: 1.7330\n",
            "Epoch [105/200], Loss: 1.7309\n",
            "Epoch [106/200], Loss: 1.5945\n",
            "Epoch [107/200], Loss: 1.6760\n",
            "Epoch [108/200], Loss: 1.5828\n",
            "Epoch [109/200], Loss: 1.5690\n",
            "Epoch [110/200], Loss: 1.5149\n",
            "Epoch [111/200], Loss: 1.4717\n",
            "Epoch [112/200], Loss: 1.6740\n",
            "Epoch [113/200], Loss: 1.5472\n",
            "Epoch [114/200], Loss: 1.5712\n",
            "Epoch [115/200], Loss: 1.5745\n",
            "Epoch [116/200], Loss: 1.5472\n",
            "Epoch [117/200], Loss: 1.6977\n",
            "Epoch [118/200], Loss: 1.5579\n",
            "Epoch [119/200], Loss: 1.4737\n",
            "Epoch [120/200], Loss: 1.4952\n",
            "Epoch [121/200], Loss: 1.5739\n",
            "Epoch [122/200], Loss: 1.5699\n",
            "Epoch [123/200], Loss: 1.6224\n",
            "Epoch [124/200], Loss: 1.5648\n",
            "Epoch [125/200], Loss: 1.5329\n",
            "Epoch [126/200], Loss: 1.5794\n",
            "Epoch [127/200], Loss: 1.5690\n",
            "Epoch [128/200], Loss: 1.6283\n",
            "Epoch [129/200], Loss: 1.5978\n",
            "Epoch [130/200], Loss: 1.5418\n",
            "Epoch [131/200], Loss: 1.5578\n",
            "Epoch [132/200], Loss: 1.6807\n",
            "Epoch [133/200], Loss: 1.5996\n",
            "Epoch [134/200], Loss: 1.6020\n",
            "Epoch [135/200], Loss: 1.6090\n",
            "Epoch [136/200], Loss: 1.6213\n",
            "Epoch [137/200], Loss: 1.6242\n",
            "Epoch [138/200], Loss: 1.6108\n",
            "Epoch [139/200], Loss: 1.6167\n",
            "Epoch [140/200], Loss: 1.4918\n",
            "Epoch [141/200], Loss: 1.5222\n",
            "Epoch [142/200], Loss: 1.5462\n",
            "Epoch [143/200], Loss: 1.5241\n",
            "Epoch [144/200], Loss: 1.5882\n",
            "Epoch [145/200], Loss: 1.5610\n",
            "Epoch [146/200], Loss: 1.5410\n",
            "Epoch [147/200], Loss: 1.5189\n",
            "Epoch [148/200], Loss: 1.5576\n",
            "Epoch [149/200], Loss: 1.5000\n",
            "Epoch [150/200], Loss: 1.5454\n",
            "Epoch [151/200], Loss: 1.6308\n",
            "Epoch [152/200], Loss: 1.6005\n",
            "Epoch [153/200], Loss: 1.6266\n",
            "Epoch [154/200], Loss: 1.5369\n",
            "Epoch [155/200], Loss: 1.5639\n",
            "Epoch [156/200], Loss: 1.6693\n",
            "Epoch [157/200], Loss: 1.5547\n",
            "Epoch [158/200], Loss: 1.5451\n",
            "Epoch [159/200], Loss: 1.6212\n",
            "Epoch [160/200], Loss: 1.5880\n",
            "Epoch [161/200], Loss: 1.4624\n",
            "Epoch [162/200], Loss: 1.4626\n",
            "Epoch [163/200], Loss: 1.5307\n",
            "Epoch [164/200], Loss: 1.5666\n",
            "Epoch [165/200], Loss: 1.5660\n",
            "Epoch [166/200], Loss: 1.4853\n",
            "Epoch [167/200], Loss: 1.5628\n",
            "Epoch [168/200], Loss: 1.6438\n",
            "Epoch [169/200], Loss: 1.6874\n",
            "Epoch [170/200], Loss: 1.5489\n",
            "Epoch [171/200], Loss: 1.6286\n",
            "Epoch [172/200], Loss: 1.5887\n",
            "Epoch [173/200], Loss: 1.5798\n",
            "Epoch [174/200], Loss: 1.5521\n",
            "Epoch [175/200], Loss: 1.5824\n",
            "Epoch [176/200], Loss: 1.5411\n",
            "Epoch [177/200], Loss: 1.6626\n",
            "Epoch [178/200], Loss: 1.5400\n",
            "Epoch [179/200], Loss: 1.5445\n",
            "Epoch [180/200], Loss: 1.5059\n",
            "Epoch [181/200], Loss: 1.6343\n",
            "Epoch [182/200], Loss: 1.6241\n",
            "Epoch [183/200], Loss: 1.6118\n",
            "Epoch [184/200], Loss: 1.5290\n",
            "Epoch [185/200], Loss: 1.5737\n",
            "Epoch [186/200], Loss: 1.6348\n",
            "Epoch [187/200], Loss: 1.4696\n",
            "Epoch [188/200], Loss: 1.5489\n",
            "Epoch [189/200], Loss: 1.5267\n",
            "Epoch [190/200], Loss: 1.5491\n",
            "Epoch [191/200], Loss: 1.6646\n",
            "Epoch [192/200], Loss: 1.5888\n",
            "Epoch [193/200], Loss: 1.5702\n",
            "Epoch [194/200], Loss: 1.5852\n",
            "Epoch [195/200], Loss: 1.5059\n",
            "Epoch [196/200], Loss: 1.4629\n",
            "Epoch [197/200], Loss: 1.5868\n",
            "Epoch [198/200], Loss: 1.6195\n",
            "Epoch [199/200], Loss: 1.5058\n",
            "Epoch [200/200], Loss: 1.5112\n",
            "Accuracy: 0.8373\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.85      0.82      0.84       208\n",
            "   classical       0.91      0.94      0.93       203\n",
            "     country       0.75      0.81      0.78       186\n",
            "       disco       0.71      0.80      0.75       199\n",
            "      hiphop       0.95      0.81      0.87       218\n",
            "        jazz       0.86      0.85      0.86       192\n",
            "       metal       0.91      0.91      0.91       204\n",
            "         pop       0.82      0.91      0.86       180\n",
            "      reggae       0.79      0.85      0.82       211\n",
            "        rock       0.84      0.67      0.74       197\n",
            "\n",
            "    accuracy                           0.84      1998\n",
            "   macro avg       0.84      0.84      0.84      1998\n",
            "weighted avg       0.84      0.84      0.84      1998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 512\n",
        "num_classes = len(label_encoder.classes_)\n",
        "learning_rate = 0.002\n",
        "num_epochs = 200\n",
        "batch_size = 32\n",
        "\n",
        "# Initializes model, loss function, and optimizer\n",
        "model = Classifier(input_size, hidden_size1, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Evaluates the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    probabilities, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Decode labels\n",
        "    y_test_decoded = label_encoder.inverse_transform(y_test_tensor)\n",
        "    predicted_decoded = label_encoder.inverse_transform(predicted)\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test_decoded, predicted_decoded))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}